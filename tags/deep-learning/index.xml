<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning on Nan Xiao | 肖楠</title>
    <link>https://nanx.me/tags/deep-learning/</link>
    <description>Recent content in deep learning on Nan Xiao | 肖楠</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 13 Jun 2025 01:03:00 +0000</lastBuildDate>
    <atom:link href="https://nanx.me/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Refactoring the GPU Selector App with Claude Code: A Vibe Coding Experiment</title>
      <link>https://nanx.me/blog/post/claude-code-vibe-refactor/</link>
      <pubDate>Fri, 13 Jun 2025 01:03:00 +0000</pubDate>
      <guid>https://nanx.me/blog/post/claude-code-vibe-refactor/</guid>
      <description>Desk setup with plants. Photo by Amari James. In January 2023, I built a basic, interactive deep learning GPU selector app inspired by Tim Dettmers’ popular guide. The original project was a pure HTML + JavaScript + CSS “vibe coding” experiment, driven by chat with GPT-3.5, which was the SOTA at the time. While the core logic worked, I did not focus much on its UI or UX.&#xA;Now, two and a half years later, I wanted to see how far agentic coding assistants have come, by refactoring the app with modern tools and design sensibilities.</description>
    </item>
    <item>
      <title>tinytopics: GPU-accelerated topic modeling via constrained neural Poisson NMF</title>
      <link>https://nanx.me/blog/post/tinytopics/</link>
      <pubDate>Sat, 26 Oct 2024 20:42:00 +0000</pubDate>
      <guid>https://nanx.me/blog/post/tinytopics/</guid>
      <description>Downhill mountain biking. Photo by Tim Foster. I’m excited to share that my first Python package, tinytopics, is now available on PyPI. You can install it using&#xA;pip install tinytopics tinytopics is a minimalist solution designed to scale up topic modeling tasks on CPUs and GPUs using PyTorch.&#xA;Motivation Fitting topic models at scale using classical algorithms on CPUs can be slow. Carbonetto et al. (2022) demonstrated the equivalence between Poisson non-negative matrix factorization (NMF) and multinomial topic model likelihoods.</description>
    </item>
    <item>
      <title>Deep Learning GPU Selector</title>
      <link>https://nanx.me/blog/post/gpu-selector/</link>
      <pubDate>Sun, 29 Jan 2023 22:00:00 +0000</pubDate>
      <guid>https://nanx.me/blog/post/gpu-selector/</guid>
      <description>I built a GPU selector for deep learning workflows at nanx.me/gpu/, based on the recommendations from the blog post written by Tim Dettmers.&#xA;You can use this tool to decide the GPU you need for general training or inference needs by answering a few yes/no questions. It was a simple, pure client-side JavaScript implementation (GitHub repo).&#xA;AI and sustainability. Artwork from DeepMind. Artist: Nidia Dias. I first saw Tim’s post when the Ampere series was released in 2020.</description>
    </item>
    <item>
      <title>Implementing Triplet Losses for Implicit Feedback Recommender Systems with R and Keras</title>
      <link>https://nanx.me/blog/post/triplet-loss-r-keras/</link>
      <pubDate>Wed, 29 Aug 2018 19:30:00 +0000</pubDate>
      <guid>https://nanx.me/blog/post/triplet-loss-r-keras/</guid>
      <description>All the R code for this post is available on GitHub: nanxstats/deep-learning-recipes.&#xA;Photo: Three Palms by Jamie Davies&#xA;At the end of our last post, I briefly mentioned that the triplet loss function is a more proper loss designed for both recommendation problems with implicit feedback data and distance metric learning problems. For its importance in solving these practical problems, and also as an excellent programming exercise, I decided to implement it with R and Keras.</description>
    </item>
    <item>
      <title>Prototyping a Recommender System for Binary Implicit Feedback Data with R and Keras</title>
      <link>https://nanx.me/blog/post/recsys-binary-implicit-feedback-r-keras/</link>
      <pubDate>Wed, 22 Aug 2018 17:30:00 +0000</pubDate>
      <guid>https://nanx.me/blog/post/recsys-binary-implicit-feedback-r-keras/</guid>
      <description>Ten years ago, the Netflix prize competition made a significant impact on recommender systems research. In the same time, such benchmark datasets, including MovieLens, are a bit misleading: in reality, implicit feedback data, or binary implicit feedback data (someone interacted with something) could be the best we can have. One to five star ratings type of continuous response data could be challenging to get or impossible to measure.&#xA;Photo: One in A Million by Veronica Benavides</description>
    </item>
    <item>
      <title>Building My First Deep Learning Machine</title>
      <link>https://nanx.me/blog/post/building-my-first-deep-learning-machine/</link>
      <pubDate>Mon, 20 Aug 2018 15:00:00 +0000</pubDate>
      <guid>https://nanx.me/blog/post/building-my-first-deep-learning-machine/</guid>
      <description>Five years ago, GNOME 3’s immaturity was one of the reasons for me to switch to a Mac. After finished watching two seasons of Mr. Robot, I somehow missed it. With my AWS/GCP bills going up quite a bit recently, I quickly decided to invest a little and build a PC to retry the Linux desktop environment, also, to tackle some personal machine learning projects.&#xA;Hardware Here is my PCPartPicker part list for the parts of choice.</description>
    </item>
    <item>
      <title>10 Things That Matter in Deep Learning (1/5)</title>
      <link>https://nanx.me/blog/post/10-things-matter-in-deep-learning-1/</link>
      <pubDate>Thu, 12 Apr 2018 17:00:00 +0000</pubDate>
      <guid>https://nanx.me/blog/post/10-things-matter-in-deep-learning-1/</guid>
      <description>Heart of the Matter by Otis Kaye, 1963. The Art Institute of Chicago. Admit it or not, neural networks (or more broadly, connectionism) are being revived to readdress almost all problems in the machine learning world. Before deep learning, there were a few much smaller hypes like matrix factorization, sparse learning, and kernel methods, but none of them is comparable to this one. Never before has a learning method been so blessed by capital, yet so cursed by it.</description>
    </item>
    <item>
      <title>Buzzword Map, or Buzz World Map</title>
      <link>https://nanx.me/blog/post/buzzword-map-or-buzz-world-map/</link>
      <pubDate>Sat, 13 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://nanx.me/blog/post/buzzword-map-or-buzz-world-map/</guid>
      <description>I don’t always care about popular topics because they usually fade fast. But when I do, here is a 2017 world interest map for deep learning, artificial intelligence, and blockchain (courtesy of Google Trends, where you can reproduce it).&#xA;Figure: Google Trends — comparing the search traffic in 2017.&#xA;The dominant color shown in each region represents the most searched term there. The question is, what does this map tell us — everything, or nothing at all?</description>
    </item>
  </channel>
</rss>
