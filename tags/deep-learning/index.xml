<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning on Nan Xiao | 肖楠</title>
    <link>https://nanx.me/tags/deep-learning/</link>
    <description>Recent content in deep learning on Nan Xiao | 肖楠</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 29 Jan 2023 22:00:00 +0000</lastBuildDate><atom:link href="https://nanx.me/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deep Learning GPU Selector</title>
      <link>https://nanx.me/blog/post/gpu-selector/</link>
      <pubDate>Sun, 29 Jan 2023 22:00:00 +0000</pubDate>
      
      <guid>https://nanx.me/blog/post/gpu-selector/</guid>
      <description>I built a GPU selector for deep learning workflows at nanx.me/gpu/, based on the recommendations from the blog post written by Tim Dettmers.
You can use this tool to decide the GPU you need for general training or inference needs by answering a few yes/no questions. It was a simple, pure client-side JavaScript implementation (GitHub repo).
AI and sustainability. Artwork from DeepMind. Artist: Nidia Dias. I first saw Tim’s post when the Ampere series was released in 2020.</description>
    </item>
    
    <item>
      <title>Implementing Triplet Losses for Implicit Feedback Recommender Systems with R and Keras</title>
      <link>https://nanx.me/blog/post/triplet-loss-r-keras/</link>
      <pubDate>Wed, 29 Aug 2018 19:30:00 +0000</pubDate>
      
      <guid>https://nanx.me/blog/post/triplet-loss-r-keras/</guid>
      <description>All the R code for this post is available on GitHub: nanxstats/deep-learning-recipes.
Photo: Three Palms by Jamie Davies
At the end of our last post, I briefly mentioned that the triplet loss function is a more proper loss designed for both recommendation problems with implicit feedback data and distance metric learning problems. For its importance in solving these practical problems, and also as an excellent programming exercise, I decided to implement it with R and Keras.</description>
    </item>
    
    <item>
      <title>Prototyping a Recommender System for Binary Implicit Feedback Data with R and Keras</title>
      <link>https://nanx.me/blog/post/recsys-binary-implicit-feedback-r-keras/</link>
      <pubDate>Wed, 22 Aug 2018 17:30:00 +0000</pubDate>
      
      <guid>https://nanx.me/blog/post/recsys-binary-implicit-feedback-r-keras/</guid>
      <description>Ten years ago, the Netflix prize competition made a significant impact on recommender systems research. In the same time, such benchmark datasets, including MovieLens, are a bit misleading: in reality, implicit feedback data, or binary implicit feedback data (someone interacted with something) could be the best we can have. One to five star ratings type of continuous response data could be challenging to get or impossible to measure.
Photo: One in A Million by Veronica Benavides</description>
    </item>
    
    <item>
      <title>Building My First Deep Learning Machine</title>
      <link>https://nanx.me/blog/post/building-my-first-deep-learning-machine/</link>
      <pubDate>Mon, 20 Aug 2018 15:00:00 +0000</pubDate>
      
      <guid>https://nanx.me/blog/post/building-my-first-deep-learning-machine/</guid>
      <description>Five years ago, GNOME 3’s immaturity was one of the reasons for me to switch to a Mac. After finished watching two seasons of Mr. Robot, I somehow missed it. With my AWS/GCP bills going up quite a bit recently, I quickly decided to invest a little and build a PC to retry the Linux desktop environment, also, to tackle some personal machine learning projects.
Hardware Here is my PCPartPicker part list for the parts of choice.</description>
    </item>
    
    <item>
      <title>10 Things That Matter in Deep Learning (1/5)</title>
      <link>https://nanx.me/blog/post/10-things-matter-in-deep-learning-1/</link>
      <pubDate>Thu, 12 Apr 2018 17:00:00 +0000</pubDate>
      
      <guid>https://nanx.me/blog/post/10-things-matter-in-deep-learning-1/</guid>
      <description>Heart of the Matter by Otis Kaye, 1963. The Art Institute of Chicago.
Admit it or not, neural networks (or more broadly, connectionism) are being revived to readdress almost all problems in the machine learning world. Before deep learning, there were a few much smaller hypes like matrix factorization, sparse learning, and kernel methods, but none of them is comparable to this one. Never before has a learning method been so blessed by capital, yet so cursed by it.</description>
    </item>
    
    <item>
      <title>Buzzword Map, or Buzz World Map</title>
      <link>https://nanx.me/blog/post/buzzword-map-or-buzz-world-map/</link>
      <pubDate>Sat, 13 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://nanx.me/blog/post/buzzword-map-or-buzz-world-map/</guid>
      <description>I don’t always care about popular topics because they usually fade fast. But when I do, here is a 2017 world interest map for deep learning, artificial intelligence, and blockchain (courtesy of Google Trends, where you can reproduce it).
Figure: Google Trends — comparing the search traffic in 2017.
The dominant color shown in each region represents the most searched term there. The question is, what does this map tell us — everything, or nothing at all?</description>
    </item>
    
  </channel>
</rss>
