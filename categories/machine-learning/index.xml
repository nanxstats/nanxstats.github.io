<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning on Nan Xiao | 肖楠</title>
    <link>https://nanx.me/categories/machine-learning/</link>
    <description>Recent content in machine learning on Nan Xiao | 肖楠</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 06 Oct 2019 00:30:00 +0000</lastBuildDate><atom:link href="https://nanx.me/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Building Regularized Logistic Regressions from Scratch with Computational Graphs in R</title>
      <link>https://nanx.me/blog/post/cgraph-logreg/</link>
      <pubDate>Sun, 06 Oct 2019 00:30:00 +0000</pubDate>
      
      <guid>https://nanx.me/blog/post/cgraph-logreg/</guid>
      <description>The R package built for this post is available on GitHub: nanxstats/logreg.
Update 2020-02-11: there has been some major API updates and improvements since cgraph 5.0.0. The logreg package is now updated to reflect these changes. The code example in this post is updated, too.
As one of the cornerstones for deep learning frameworks, automatic differentiation was briefly mentioned in our previous post. Today let’s focus on the other important piece: the computational graph.</description>
    </item>
    
    <item>
      <title>64GB RAM</title>
      <link>https://nanx.me/blog/post/64gb-ram/</link>
      <pubDate>Tue, 11 Jun 2019 22:30:00 +0000</pubDate>
      
      <guid>https://nanx.me/blog/post/64gb-ram/</guid>
      <description>In 2018, I built a PC to train models with GPU. Besides the GPU, the machine has a 6-core (12 threads) CPU and 32GB of RAM. The hardware was nothing fancy compared to a lot of deep learning builds but gets the job done.
Recently, I experienced an issue that probably all R users experienced: running out of memory. Not for training neural networks, but for training some more traditional statistical learning models.</description>
    </item>
    
    <item>
      <title>Implementing Triplet Losses for Implicit Feedback Recommender Systems with R and Keras</title>
      <link>https://nanx.me/blog/post/triplet-loss-r-keras/</link>
      <pubDate>Wed, 29 Aug 2018 19:30:00 +0000</pubDate>
      
      <guid>https://nanx.me/blog/post/triplet-loss-r-keras/</guid>
      <description>All the R code for this post is available on GitHub: nanxstats/deep-learning-recipes.
Photo: Three Palms by Jamie Davies
 At the end of our last post, I briefly mentioned that the triplet loss function is a more proper loss designed for both recommendation problems with implicit feedback data and distance metric learning problems. For its importance in solving these practical problems, and also as an excellent programming exercise, I decided to implement it with R and Keras.</description>
    </item>
    
    <item>
      <title>Prototyping a Recommender System for Binary Implicit Feedback Data with R and Keras</title>
      <link>https://nanx.me/blog/post/recsys-binary-implicit-feedback-r-keras/</link>
      <pubDate>Wed, 22 Aug 2018 17:30:00 +0000</pubDate>
      
      <guid>https://nanx.me/blog/post/recsys-binary-implicit-feedback-r-keras/</guid>
      <description>Ten years ago, the Netflix prize competition made a significant impact on recommender systems research. In the same time, such benchmark datasets, including MovieLens, are a bit misleading: in reality, implicit feedback data, or binary implicit feedback data (someone interacted with something) could be the best we can have. One to five star ratings type of continuous response data could be challenging to get or impossible to measure.</description>
    </item>
    
    <item>
      <title>Building My First Deep Learning Machine</title>
      <link>https://nanx.me/blog/post/building-my-first-deep-learning-machine/</link>
      <pubDate>Mon, 20 Aug 2018 15:00:00 +0000</pubDate>
      
      <guid>https://nanx.me/blog/post/building-my-first-deep-learning-machine/</guid>
      <description>Five years ago, GNOME 3’s immaturity was one of the reasons for me to switch to a Mac. After finished watching two seasons of Mr. Robot, I somehow missed it. With my AWS/GCP bills going up quite a bit recently, I quickly decided to invest a little and build a PC to retry the Linux desktop environment, also, to tackle some personal machine learning projects.
Hardware Here is my PCPartPicker part list for the parts of choice.</description>
    </item>
    
    <item>
      <title>10 Things That Matter in Deep Learning (1/5)</title>
      <link>https://nanx.me/blog/post/10-things-matter-in-deep-learning-1/</link>
      <pubDate>Thu, 12 Apr 2018 17:00:00 +0000</pubDate>
      
      <guid>https://nanx.me/blog/post/10-things-matter-in-deep-learning-1/</guid>
      <description>Heart of the Matter by Otis Kaye, 1963. The Art Institute of Chicago.
 Admit it or not, neural networks (or more broadly, connectionism) are being revived to readdress almost all problems in the machine learning world. Before deep learning, there were a few much smaller hypes like matrix factorization, sparse learning, and kernel methods, but none of them is comparable to this one. Never before has a learning method been so blessed by capital, yet so cursed by it.</description>
    </item>
    
  </channel>
</rss>
